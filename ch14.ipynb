{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 2.9423 - accuracy: 0.7519 - val_loss: 2.2360 - val_accuracy: 0.7562\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 2.0859 - accuracy: 0.76 - 0s 6ms/step - loss: 1.7741 - accuracy: 0.7729 - val_loss: 1.3866 - val_accuracy: 0.7892\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0029 - accuracy: 0.8181 - val_loss: 0.6412 - val_accuracy: 0.8469\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4439 - accuracy: 0.8571 - val_loss: 0.4369 - val_accuracy: 0.8062\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4373 - accuracy: 0.8011 - val_loss: 0.3428 - val_accuracy: 0.8585\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3310 - accuracy: 0.8758 - val_loss: 0.3595 - val_accuracy: 0.8808\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8861 - val_loss: 0.3108 - val_accuracy: 0.8869\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2782 - accuracy: 0.8891 - val_loss: 0.2877 - val_accuracy: 0.8769\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2509 - accuracy: 0.9012 - val_loss: 0.2469 - val_accuracy: 0.9108\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2162 - accuracy: 0.9264 - val_loss: 0.2369 - val_accuracy: 0.9200\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1951 - accuracy: 0.9361 - val_loss: 0.2283 - val_accuracy: 0.9177\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1892 - accuracy: 0.9369 - val_loss: 0.2269 - val_accuracy: 0.9246\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9361 - val_loss: 0.2160 - val_accuracy: 0.9269\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9374 - val_loss: 0.2083 - val_accuracy: 0.9277\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1740 - accuracy: 0.9366 - val_loss: 0.2016 - val_accuracy: 0.9277\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1708 - accuracy: 0.9371 - val_loss: 0.1986 - val_accuracy: 0.9285\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1682 - accuracy: 0.9356 - val_loss: 0.1956 - val_accuracy: 0.9300\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1663 - accuracy: 0.9371 - val_loss: 0.1965 - val_accuracy: 0.9308\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9379 - val_loss: 0.1916 - val_accuracy: 0.9285\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.9397 - val_loss: 0.1906 - val_accuracy: 0.9315\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1600 - accuracy: 0.9397 - val_loss: 0.1877 - val_accuracy: 0.9308\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9389 - val_loss: 0.1874 - val_accuracy: 0.9331\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9410 - val_loss: 0.1875 - val_accuracy: 0.9323\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9418 - val_loss: 0.1841 - val_accuracy: 0.9323\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1539 - accuracy: 0.9410 - val_loss: 0.1800 - val_accuracy: 0.9338\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9433 - val_loss: 0.1836 - val_accuracy: 0.9331\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1503 - accuracy: 0.9428 - val_loss: 0.1795 - val_accuracy: 0.9354\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9441 - val_loss: 0.1775 - val_accuracy: 0.9392\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9443 - val_loss: 0.1784 - val_accuracy: 0.9338\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1436 - accuracy: 0.9469 - val_loss: 0.1735 - val_accuracy: 0.9377\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9482 - val_loss: 0.1751 - val_accuracy: 0.9438\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9484 - val_loss: 0.1798 - val_accuracy: 0.9346\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1410 - accuracy: 0.9469 - val_loss: 0.1727 - val_accuracy: 0.9377\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9482 - val_loss: 0.1673 - val_accuracy: 0.9415\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1363 - accuracy: 0.9494 - val_loss: 0.1660 - val_accuracy: 0.9415\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1340 - accuracy: 0.9505 - val_loss: 0.1656 - val_accuracy: 0.9438\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9502 - val_loss: 0.1699 - val_accuracy: 0.9392\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9518 - val_loss: 0.1665 - val_accuracy: 0.9408\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.9525 - val_loss: 0.1619 - val_accuracy: 0.9462\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1286 - accuracy: 0.9515 - val_loss: 0.1610 - val_accuracy: 0.9469\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9515 - val_loss: 0.1602 - val_accuracy: 0.9477\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1256 - accuracy: 0.9523 - val_loss: 0.1616 - val_accuracy: 0.9431\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1258 - accuracy: 0.9518 - val_loss: 0.1619 - val_accuracy: 0.9431\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9541 - val_loss: 0.1629 - val_accuracy: 0.9423\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.9530 - val_loss: 0.1570 - val_accuracy: 0.9477\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9525 - val_loss: 0.1558 - val_accuracy: 0.9485\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9543 - val_loss: 0.1557 - val_accuracy: 0.9485\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9556 - val_loss: 0.1552 - val_accuracy: 0.9462\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9564 - val_loss: 0.1593 - val_accuracy: 0.9446\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1161 - accuracy: 0.9574 - val_loss: 0.1523 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9415\n",
      "Test accuracy: 0.9415384531021118\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to ../data/model/all\\01-0.7646.hdf5\n",
      "\n",
      "Epoch 00002: saving model to ../data/model/all\\02-0.7646.hdf5\n",
      "\n",
      "Epoch 00003: saving model to ../data/model/all\\03-0.7646.hdf5\n",
      "\n",
      "Epoch 00004: saving model to ../data/model/all\\04-0.7646.hdf5\n",
      "\n",
      "Epoch 00005: saving model to ../data/model/all\\05-0.7646.hdf5\n",
      "\n",
      "Epoch 00006: saving model to ../data/model/all\\06-0.7646.hdf5\n",
      "\n",
      "Epoch 00007: saving model to ../data/model/all\\07-0.8669.hdf5\n",
      "\n",
      "Epoch 00008: saving model to ../data/model/all\\08-0.8900.hdf5\n",
      "\n",
      "Epoch 00009: saving model to ../data/model/all\\09-0.9031.hdf5\n",
      "\n",
      "Epoch 00010: saving model to ../data/model/all\\10-0.9031.hdf5\n",
      "\n",
      "Epoch 00011: saving model to ../data/model/all\\11-0.9085.hdf5\n",
      "\n",
      "Epoch 00012: saving model to ../data/model/all\\12-0.9131.hdf5\n",
      "\n",
      "Epoch 00013: saving model to ../data/model/all\\13-0.9177.hdf5\n",
      "\n",
      "Epoch 00014: saving model to ../data/model/all\\14-0.9215.hdf5\n",
      "\n",
      "Epoch 00015: saving model to ../data/model/all\\15-0.9277.hdf5\n",
      "\n",
      "Epoch 00016: saving model to ../data/model/all\\16-0.9285.hdf5\n",
      "\n",
      "Epoch 00017: saving model to ../data/model/all\\17-0.9269.hdf5\n",
      "\n",
      "Epoch 00018: saving model to ../data/model/all\\18-0.9177.hdf5\n",
      "\n",
      "Epoch 00019: saving model to ../data/model/all\\19-0.9246.hdf5\n",
      "\n",
      "Epoch 00020: saving model to ../data/model/all\\20-0.9331.hdf5\n",
      "\n",
      "Epoch 00021: saving model to ../data/model/all\\21-0.9331.hdf5\n",
      "\n",
      "Epoch 00022: saving model to ../data/model/all\\22-0.9331.hdf5\n",
      "\n",
      "Epoch 00023: saving model to ../data/model/all\\23-0.9338.hdf5\n",
      "\n",
      "Epoch 00024: saving model to ../data/model/all\\24-0.9338.hdf5\n",
      "\n",
      "Epoch 00025: saving model to ../data/model/all\\25-0.9331.hdf5\n",
      "\n",
      "Epoch 00026: saving model to ../data/model/all\\26-0.9323.hdf5\n",
      "\n",
      "Epoch 00027: saving model to ../data/model/all\\27-0.9338.hdf5\n",
      "\n",
      "Epoch 00028: saving model to ../data/model/all\\28-0.9338.hdf5\n",
      "\n",
      "Epoch 00029: saving model to ../data/model/all\\29-0.9338.hdf5\n",
      "\n",
      "Epoch 00030: saving model to ../data/model/all\\30-0.9346.hdf5\n",
      "\n",
      "Epoch 00031: saving model to ../data/model/all\\31-0.9354.hdf5\n",
      "\n",
      "Epoch 00032: saving model to ../data/model/all\\32-0.9377.hdf5\n",
      "\n",
      "Epoch 00033: saving model to ../data/model/all\\33-0.9377.hdf5\n",
      "\n",
      "Epoch 00034: saving model to ../data/model/all\\34-0.9385.hdf5\n",
      "\n",
      "Epoch 00035: saving model to ../data/model/all\\35-0.9385.hdf5\n",
      "\n",
      "Epoch 00036: saving model to ../data/model/all\\36-0.9392.hdf5\n",
      "\n",
      "Epoch 00037: saving model to ../data/model/all\\37-0.9385.hdf5\n",
      "\n",
      "Epoch 00038: saving model to ../data/model/all\\38-0.9392.hdf5\n",
      "\n",
      "Epoch 00039: saving model to ../data/model/all\\39-0.9385.hdf5\n",
      "\n",
      "Epoch 00040: saving model to ../data/model/all\\40-0.9385.hdf5\n",
      "\n",
      "Epoch 00041: saving model to ../data/model/all\\41-0.9392.hdf5\n",
      "\n",
      "Epoch 00042: saving model to ../data/model/all\\42-0.9369.hdf5\n",
      "\n",
      "Epoch 00043: saving model to ../data/model/all\\43-0.9392.hdf5\n",
      "\n",
      "Epoch 00044: saving model to ../data/model/all\\44-0.9392.hdf5\n",
      "\n",
      "Epoch 00045: saving model to ../data/model/all\\45-0.9392.hdf5\n",
      "\n",
      "Epoch 00046: saving model to ../data/model/all\\46-0.9400.hdf5\n",
      "\n",
      "Epoch 00047: saving model to ../data/model/all\\47-0.9408.hdf5\n",
      "\n",
      "Epoch 00048: saving model to ../data/model/all\\48-0.9408.hdf5\n",
      "\n",
      "Epoch 00049: saving model to ../data/model/all\\49-0.9408.hdf5\n",
      "\n",
      "Epoch 00050: saving model to ../data/model/all\\50-0.9408.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.9392\n",
      "Test accuracy: 0.939230740070343\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147854</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.163222</td>\n",
       "      <td>0.938462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146364</td>\n",
       "      <td>0.948165</td>\n",
       "      <td>0.161854</td>\n",
       "      <td>0.940769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145011</td>\n",
       "      <td>0.947909</td>\n",
       "      <td>0.160797</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.144104</td>\n",
       "      <td>0.947909</td>\n",
       "      <td>0.158959</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142609</td>\n",
       "      <td>0.948935</td>\n",
       "      <td>0.159250</td>\n",
       "      <td>0.940769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.030691</td>\n",
       "      <td>0.992045</td>\n",
       "      <td>0.071841</td>\n",
       "      <td>0.983077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.030978</td>\n",
       "      <td>0.992558</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.030977</td>\n",
       "      <td>0.992045</td>\n",
       "      <td>0.074907</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.032843</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.078405</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.033346</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.068213</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.147854  0.947395  0.163222      0.938462\n",
       "1     0.146364  0.948165  0.161854      0.940769\n",
       "2     0.145011  0.947909  0.160797      0.940000\n",
       "3     0.144104  0.947909  0.158959      0.946154\n",
       "4     0.142609  0.948935  0.159250      0.940769\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.030691  0.992045  0.071841      0.983077\n",
       "1996  0.030978  0.992558  0.070400      0.985385\n",
       "1997  0.030977  0.992045  0.074907      0.985385\n",
       "1998  0.032843  0.990762  0.078405      0.984615\n",
       "1999  0.033346  0.990762  0.068213      0.984615\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 21.4771 - accuracy: 0.2494 - val_loss: 14.8183 - val_accuracy: 0.2462\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.2518 - accuracy: 0.2494 - val_loss: 7.2853 - val_accuracy: 0.2462\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9423 - accuracy: 0.2476 - val_loss: 1.8426 - val_accuracy: 0.2200\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7835 - accuracy: 0.6074 - val_loss: 0.3719 - val_accuracy: 0.8031\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8004 - val_loss: 0.4128 - val_accuracy: 0.8092\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.8163 - val_loss: 0.3641 - val_accuracy: 0.8300\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3622 - accuracy: 0.8399 - val_loss: 0.2913 - val_accuracy: 0.8692\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8799 - val_loss: 0.3087 - val_accuracy: 0.9000\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3129 - accuracy: 0.8835 - val_loss: 0.2807 - val_accuracy: 0.8900\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3031 - accuracy: 0.8737 - val_loss: 0.2762 - val_accuracy: 0.8908\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2984 - accuracy: 0.8812 - val_loss: 0.2730 - val_accuracy: 0.9015\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2931 - accuracy: 0.8909 - val_loss: 0.2693 - val_accuracy: 0.9085\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2887 - accuracy: 0.8945 - val_loss: 0.2633 - val_accuracy: 0.9108\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2845 - accuracy: 0.8958 - val_loss: 0.2577 - val_accuracy: 0.9123\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2801 - accuracy: 0.8984 - val_loss: 0.2529 - val_accuracy: 0.9162\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.9038 - val_loss: 0.2461 - val_accuracy: 0.9208\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2662 - accuracy: 0.9112 - val_loss: 0.2346 - val_accuracy: 0.9277\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2577 - accuracy: 0.9197 - val_loss: 0.2285 - val_accuracy: 0.9308\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2539 - accuracy: 0.9202 - val_loss: 0.2227 - val_accuracy: 0.9308\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2496 - accuracy: 0.9212 - val_loss: 0.2175 - val_accuracy: 0.9315\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2455 - accuracy: 0.9215 - val_loss: 0.2134 - val_accuracy: 0.9338\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2418 - accuracy: 0.9212 - val_loss: 0.2092 - val_accuracy: 0.9346\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9212 - val_loss: 0.2057 - val_accuracy: 0.9354\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2347 - accuracy: 0.9220 - val_loss: 0.2018 - val_accuracy: 0.9354\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2310 - accuracy: 0.9243 - val_loss: 0.1990 - val_accuracy: 0.9346\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9240 - val_loss: 0.1952 - val_accuracy: 0.9369\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2237 - accuracy: 0.9246 - val_loss: 0.1920 - val_accuracy: 0.9377\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9261 - val_loss: 0.1899 - val_accuracy: 0.9377\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2183 - accuracy: 0.9264 - val_loss: 0.1872 - val_accuracy: 0.9377\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2158 - accuracy: 0.9271 - val_loss: 0.1835 - val_accuracy: 0.9392\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9281 - val_loss: 0.1839 - val_accuracy: 0.9369\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2128 - accuracy: 0.9297 - val_loss: 0.1806 - val_accuracy: 0.9392\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2100 - accuracy: 0.9307 - val_loss: 0.1789 - val_accuracy: 0.9400\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2083 - accuracy: 0.9323 - val_loss: 0.1777 - val_accuracy: 0.9392\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9305 - val_loss: 0.1752 - val_accuracy: 0.9423\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2043 - accuracy: 0.9333 - val_loss: 0.1738 - val_accuracy: 0.9438\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2025 - accuracy: 0.9325 - val_loss: 0.1726 - val_accuracy: 0.9423\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2015 - accuracy: 0.9341 - val_loss: 0.1703 - val_accuracy: 0.9446\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1999 - accuracy: 0.9325 - val_loss: 0.1695 - val_accuracy: 0.9438\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1980 - accuracy: 0.9348 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9341 - val_loss: 0.1665 - val_accuracy: 0.9469\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1946 - accuracy: 0.9338 - val_loss: 0.1649 - val_accuracy: 0.9469\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1932 - accuracy: 0.9364 - val_loss: 0.1634 - val_accuracy: 0.9485\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9358 - val_loss: 0.1608 - val_accuracy: 0.9462\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9348 - val_loss: 0.1614 - val_accuracy: 0.9485\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1889 - accuracy: 0.9369 - val_loss: 0.1597 - val_accuracy: 0.9477\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1878 - accuracy: 0.9389 - val_loss: 0.1590 - val_accuracy: 0.9500\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1858 - accuracy: 0.9364 - val_loss: 0.1567 - val_accuracy: 0.9508\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1842 - accuracy: 0.9389 - val_loss: 0.1552 - val_accuracy: 0.9508\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1831 - accuracy: 0.9371 - val_loss: 0.1531 - val_accuracy: 0.9500\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.9400 - val_loss: 0.1521 - val_accuracy: 0.9508\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9389 - val_loss: 0.1497 - val_accuracy: 0.9477\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1788 - accuracy: 0.9397 - val_loss: 0.1524 - val_accuracy: 0.9508\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9407 - val_loss: 0.1473 - val_accuracy: 0.9508\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9423 - val_loss: 0.1487 - val_accuracy: 0.9515\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1712 - accuracy: 0.9420 - val_loss: 0.1436 - val_accuracy: 0.9508\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9423 - val_loss: 0.1478 - val_accuracy: 0.9500\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1677 - accuracy: 0.9423 - val_loss: 0.1410 - val_accuracy: 0.9515\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1646 - accuracy: 0.9433 - val_loss: 0.1406 - val_accuracy: 0.9515\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1625 - accuracy: 0.9425 - val_loss: 0.1380 - val_accuracy: 0.9508\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9433 - val_loss: 0.1392 - val_accuracy: 0.9515\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.9425 - val_loss: 0.1346 - val_accuracy: 0.9523\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9433 - val_loss: 0.1328 - val_accuracy: 0.9531\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1550 - accuracy: 0.9446 - val_loss: 0.1312 - val_accuracy: 0.9523\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1550 - accuracy: 0.9456 - val_loss: 0.1370 - val_accuracy: 0.9523\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9433 - val_loss: 0.1284 - val_accuracy: 0.9538\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9435 - val_loss: 0.1288 - val_accuracy: 0.9546\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1483 - accuracy: 0.9446 - val_loss: 0.1269 - val_accuracy: 0.9554\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9453 - val_loss: 0.1299 - val_accuracy: 0.9538\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9456 - val_loss: 0.1250 - val_accuracy: 0.9554\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9453 - val_loss: 0.1261 - val_accuracy: 0.9546\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1430 - accuracy: 0.9461 - val_loss: 0.1232 - val_accuracy: 0.9577\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.9471 - val_loss: 0.1210 - val_accuracy: 0.9569\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1410 - accuracy: 0.9471 - val_loss: 0.1222 - val_accuracy: 0.9600\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9489 - val_loss: 0.1196 - val_accuracy: 0.9608\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9492 - val_loss: 0.1187 - val_accuracy: 0.9608\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1366 - accuracy: 0.9492 - val_loss: 0.1167 - val_accuracy: 0.9577\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1362 - accuracy: 0.9484 - val_loss: 0.1173 - val_accuracy: 0.9608\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1345 - accuracy: 0.9500 - val_loss: 0.1200 - val_accuracy: 0.9592\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.9494 - val_loss: 0.1151 - val_accuracy: 0.9592\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9497 - val_loss: 0.1128 - val_accuracy: 0.9608\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9507 - val_loss: 0.1237 - val_accuracy: 0.9592\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9497 - val_loss: 0.1117 - val_accuracy: 0.9623\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9551 - val_loss: 0.1108 - val_accuracy: 0.9608\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.9512 - val_loss: 0.1189 - val_accuracy: 0.9623\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1301 - accuracy: 0.9523 - val_loss: 0.1122 - val_accuracy: 0.9615\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9543 - val_loss: 0.1086 - val_accuracy: 0.9631\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9541 - val_loss: 0.1085 - val_accuracy: 0.9631\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9538 - val_loss: 0.1133 - val_accuracy: 0.9638\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1236 - accuracy: 0.9546 - val_loss: 0.1067 - val_accuracy: 0.9623\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1232 - accuracy: 0.9561 - val_loss: 0.1058 - val_accuracy: 0.9631\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9546 - val_loss: 0.1085 - val_accuracy: 0.9631\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1207 - accuracy: 0.9556 - val_loss: 0.1058 - val_accuracy: 0.9677\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9579 - val_loss: 0.1070 - val_accuracy: 0.9669\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9577 - val_loss: 0.1031 - val_accuracy: 0.9638\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1186 - accuracy: 0.9569 - val_loss: 0.1048 - val_accuracy: 0.9669\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1170 - accuracy: 0.9577 - val_loss: 0.1018 - val_accuracy: 0.9646\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9577 - val_loss: 0.1028 - val_accuracy: 0.9685\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1162 - accuracy: 0.9571 - val_loss: 0.1023 - val_accuracy: 0.9677\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9615 - val_loss: 0.1001 - val_accuracy: 0.9654\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9571 - val_loss: 0.1099 - val_accuracy: 0.9662\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9574 - val_loss: 0.0994 - val_accuracy: 0.9677\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9613 - val_loss: 0.0987 - val_accuracy: 0.9638\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9597 - val_loss: 0.0999 - val_accuracy: 0.9700\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1117 - accuracy: 0.9605 - val_loss: 0.0989 - val_accuracy: 0.9708\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9618 - val_loss: 0.0975 - val_accuracy: 0.9700\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9607 - val_loss: 0.0962 - val_accuracy: 0.9700\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9620 - val_loss: 0.1003 - val_accuracy: 0.9723\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9602 - val_loss: 0.0961 - val_accuracy: 0.9708\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9633 - val_loss: 0.0948 - val_accuracy: 0.9700\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9633 - val_loss: 0.0967 - val_accuracy: 0.9731\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9633 - val_loss: 0.0948 - val_accuracy: 0.9715\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9636 - val_loss: 0.0953 - val_accuracy: 0.9731\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9636 - val_loss: 0.0932 - val_accuracy: 0.9685\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9630 - val_loss: 0.0929 - val_accuracy: 0.9723\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9600 - val_loss: 0.1002 - val_accuracy: 0.9723\n",
      "Epoch 117/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9664 - val_loss: 0.0917 - val_accuracy: 0.9715\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9646 - val_loss: 0.0965 - val_accuracy: 0.9723\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.9654 - val_loss: 0.0916 - val_accuracy: 0.9731\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9659 - val_loss: 0.0908 - val_accuracy: 0.9723\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9648 - val_loss: 0.0928 - val_accuracy: 0.9746\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9666 - val_loss: 0.0926 - val_accuracy: 0.9738\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9669 - val_loss: 0.0910 - val_accuracy: 0.9754\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0998 - accuracy: 0.9669 - val_loss: 0.0898 - val_accuracy: 0.9754\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9677 - val_loss: 0.0890 - val_accuracy: 0.9692\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9666 - val_loss: 0.0921 - val_accuracy: 0.9769\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9656 - val_loss: 0.0889 - val_accuracy: 0.9762\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9695 - val_loss: 0.0883 - val_accuracy: 0.9754\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0973 - accuracy: 0.9700 - val_loss: 0.0883 - val_accuracy: 0.9754\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9700 - val_loss: 0.0881 - val_accuracy: 0.9754\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9700 - val_loss: 0.0873 - val_accuracy: 0.9754\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9672 - val_loss: 0.0919 - val_accuracy: 0.9769\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0963 - accuracy: 0.9705 - val_loss: 0.0861 - val_accuracy: 0.9746\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0962 - accuracy: 0.9705 - val_loss: 0.0857 - val_accuracy: 0.9738\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0945 - accuracy: 0.9702 - val_loss: 0.0855 - val_accuracy: 0.9738\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0941 - accuracy: 0.9710 - val_loss: 0.0854 - val_accuracy: 0.9769\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0933 - accuracy: 0.9702 - val_loss: 0.0850 - val_accuracy: 0.9777\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9700 - val_loss: 0.0865 - val_accuracy: 0.9777\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9713 - val_loss: 0.0866 - val_accuracy: 0.9785\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0925 - accuracy: 0.9700 - val_loss: 0.0839 - val_accuracy: 0.9754\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0916 - accuracy: 0.9718 - val_loss: 0.0841 - val_accuracy: 0.9785\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0911 - accuracy: 0.9710 - val_loss: 0.0862 - val_accuracy: 0.9785\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0914 - accuracy: 0.9682 - val_loss: 0.0840 - val_accuracy: 0.9792\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9713 - val_loss: 0.0837 - val_accuracy: 0.9785\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9720 - val_loss: 0.0827 - val_accuracy: 0.9762\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0900 - accuracy: 0.9707 - val_loss: 0.0829 - val_accuracy: 0.9785\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9710 - val_loss: 0.0834 - val_accuracy: 0.9785\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0894 - accuracy: 0.9720 - val_loss: 0.0849 - val_accuracy: 0.9777\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9713 - val_loss: 0.0855 - val_accuracy: 0.9769\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9713 - val_loss: 0.0817 - val_accuracy: 0.9754\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0890 - accuracy: 0.9728 - val_loss: 0.0816 - val_accuracy: 0.9754\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.9702 - val_loss: 0.0882 - val_accuracy: 0.9769\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9723 - val_loss: 0.0803 - val_accuracy: 0.9777\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9687 - val_loss: 0.0916 - val_accuracy: 0.9746\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9697 - val_loss: 0.0813 - val_accuracy: 0.9754\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9713 - val_loss: 0.0807 - val_accuracy: 0.9762\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9687 - val_loss: 0.0955 - val_accuracy: 0.9723\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9707 - val_loss: 0.0799 - val_accuracy: 0.9792\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9710 - val_loss: 0.0832 - val_accuracy: 0.9715\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9692 - val_loss: 0.0855 - val_accuracy: 0.9777\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9710 - val_loss: 0.0804 - val_accuracy: 0.9792\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0854 - accuracy: 0.9718 - val_loss: 0.0833 - val_accuracy: 0.9715\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0876 - accuracy: 0.9707 - val_loss: 0.0805 - val_accuracy: 0.9785\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9705 - val_loss: 0.0848 - val_accuracy: 0.9762\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9731 - val_loss: 0.0785 - val_accuracy: 0.9792\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.9725 - val_loss: 0.0782 - val_accuracy: 0.9792\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9725 - val_loss: 0.0805 - val_accuracy: 0.9792\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0824 - accuracy: 0.9723 - val_loss: 0.0798 - val_accuracy: 0.9800\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9738 - val_loss: 0.0777 - val_accuracy: 0.9785\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0810 - accuracy: 0.9733 - val_loss: 0.0789 - val_accuracy: 0.9800\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.9715 - val_loss: 0.0787 - val_accuracy: 0.9800\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0809 - accuracy: 0.9733 - val_loss: 0.0773 - val_accuracy: 0.9792\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9731 - val_loss: 0.0772 - val_accuracy: 0.9785\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9736 - val_loss: 0.0779 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0798 - accuracy: 0.9723 - val_loss: 0.0766 - val_accuracy: 0.9792\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0789 - accuracy: 0.9743 - val_loss: 0.0774 - val_accuracy: 0.9792\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9733 - val_loss: 0.0779 - val_accuracy: 0.9777\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9743 - val_loss: 0.0786 - val_accuracy: 0.9792\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9749 - val_loss: 0.0811 - val_accuracy: 0.9777\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9749 - val_loss: 0.0759 - val_accuracy: 0.9785\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9736 - val_loss: 0.0759 - val_accuracy: 0.9792\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9738 - val_loss: 0.0763 - val_accuracy: 0.9800\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9741 - val_loss: 0.0765 - val_accuracy: 0.9800\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9736 - val_loss: 0.0755 - val_accuracy: 0.9792\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9738 - val_loss: 0.0754 - val_accuracy: 0.9777\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9741 - val_loss: 0.0752 - val_accuracy: 0.9792\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9749 - val_loss: 0.0773 - val_accuracy: 0.9792\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9741 - val_loss: 0.0749 - val_accuracy: 0.9800\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9746 - val_loss: 0.0752 - val_accuracy: 0.9808\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9764 - val_loss: 0.0753 - val_accuracy: 0.9792\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9756 - val_loss: 0.0745 - val_accuracy: 0.9777\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9759 - val_loss: 0.0787 - val_accuracy: 0.9777\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9751 - val_loss: 0.0746 - val_accuracy: 0.9800\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9746 - val_loss: 0.0749 - val_accuracy: 0.9777\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9756 - val_loss: 0.0738 - val_accuracy: 0.9808\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9759 - val_loss: 0.0732 - val_accuracy: 0.9800\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9759 - val_loss: 0.0780 - val_accuracy: 0.9769\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9792\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0765 - accuracy: 0.9741 - val_loss: 0.0817 - val_accuracy: 0.9769\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9741 - val_loss: 0.0738 - val_accuracy: 0.9785\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 0.0753 - val_accuracy: 0.9800\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9769 - val_loss: 0.0728 - val_accuracy: 0.9800\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9766 - val_loss: 0.0752 - val_accuracy: 0.9800\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9769 - val_loss: 0.0742 - val_accuracy: 0.9785\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9759 - val_loss: 0.0744 - val_accuracy: 0.9785\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9761 - val_loss: 0.0735 - val_accuracy: 0.9815\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9741 - val_loss: 0.0833 - val_accuracy: 0.9769\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9702 - val_loss: 0.0725 - val_accuracy: 0.9808\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0717 - accuracy: 0.9756 - val_loss: 0.0722 - val_accuracy: 0.9808\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 0.9766 - val_loss: 0.0724 - val_accuracy: 0.9808\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9772 - val_loss: 0.0714 - val_accuracy: 0.9815\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9782 - val_loss: 0.0716 - val_accuracy: 0.9800\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0678 - accuracy: 0.9782 - val_loss: 0.0715 - val_accuracy: 0.9800\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9772 - val_loss: 0.0707 - val_accuracy: 0.9800\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9774 - val_loss: 0.0750 - val_accuracy: 0.9777\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9761 - val_loss: 0.0763 - val_accuracy: 0.9769\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0712 - accuracy: 0.9736 - val_loss: 0.0711 - val_accuracy: 0.9800\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9761 - val_loss: 0.0726 - val_accuracy: 0.9800\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9784 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0708 - accuracy: 0.9751 - val_loss: 0.0716 - val_accuracy: 0.9808\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9779 - val_loss: 0.0731 - val_accuracy: 0.9785\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0698 - accuracy: 0.9769 - val_loss: 0.0697 - val_accuracy: 0.9800\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9764 - val_loss: 0.0767 - val_accuracy: 0.9769\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9769 - val_loss: 0.0704 - val_accuracy: 0.9792\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9769 - val_loss: 0.0709 - val_accuracy: 0.9800\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9777 - val_loss: 0.0717 - val_accuracy: 0.9792\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9792 - val_loss: 0.0702 - val_accuracy: 0.9808\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0651 - accuracy: 0.9772 - val_loss: 0.0697 - val_accuracy: 0.9800\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9774 - val_loss: 0.0691 - val_accuracy: 0.9800\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9772 - val_loss: 0.0703 - val_accuracy: 0.9792\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9787 - val_loss: 0.0691 - val_accuracy: 0.9808\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9774 - val_loss: 0.0699 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9777 - val_loss: 0.0760 - val_accuracy: 0.9762\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9738 - val_loss: 0.0866 - val_accuracy: 0.9746\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9710 - val_loss: 0.0723 - val_accuracy: 0.9808\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9777 - val_loss: 0.0687 - val_accuracy: 0.9800\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9787 - val_loss: 0.0685 - val_accuracy: 0.9808\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0628 - accuracy: 0.9772 - val_loss: 0.0684 - val_accuracy: 0.9800\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9792 - val_loss: 0.0687 - val_accuracy: 0.9808\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0632 - accuracy: 0.9787 - val_loss: 0.0688 - val_accuracy: 0.9808\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9782 - val_loss: 0.0685 - val_accuracy: 0.9800\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9792 - val_loss: 0.0677 - val_accuracy: 0.9815\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9790 - val_loss: 0.0681 - val_accuracy: 0.9800\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9784 - val_loss: 0.0685 - val_accuracy: 0.9800\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9792 - val_loss: 0.0696 - val_accuracy: 0.9808\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9774 - val_loss: 0.0679 - val_accuracy: 0.9800\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.0688 - val_accuracy: 0.9815\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.9766 - val_loss: 0.0675 - val_accuracy: 0.9831\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 0.0672 - val_accuracy: 0.9831\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9790 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.9782 - val_loss: 0.0714 - val_accuracy: 0.9792\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9792 - val_loss: 0.0673 - val_accuracy: 0.9808\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.0675 - val_accuracy: 0.9815\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9792 - val_loss: 0.0702 - val_accuracy: 0.9800\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0624 - accuracy: 0.9769 - val_loss: 0.0689 - val_accuracy: 0.9823\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9779 - val_loss: 0.0685 - val_accuracy: 0.9815\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9792 - val_loss: 0.0666 - val_accuracy: 0.9808\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9800 - val_loss: 0.0669 - val_accuracy: 0.9823\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0597 - accuracy: 0.9792 - val_loss: 0.0683 - val_accuracy: 0.9808\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9810 - val_loss: 0.0672 - val_accuracy: 0.9815\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9805 - val_loss: 0.0668 - val_accuracy: 0.9831\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0587 - accuracy: 0.9808 - val_loss: 0.0666 - val_accuracy: 0.9815\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9800 - val_loss: 0.0674 - val_accuracy: 0.9823\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9787 - val_loss: 0.0716 - val_accuracy: 0.9815\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9774 - val_loss: 0.0691 - val_accuracy: 0.9831\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 0.0662 - val_accuracy: 0.9823\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0590 - accuracy: 0.9802 - val_loss: 0.0667 - val_accuracy: 0.9815\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9795 - val_loss: 0.0671 - val_accuracy: 0.9823\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9810 - val_loss: 0.0658 - val_accuracy: 0.9815\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.9795 - val_loss: 0.0734 - val_accuracy: 0.9785\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9782 - val_loss: 0.0684 - val_accuracy: 0.9792\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9802 - val_loss: 0.0658 - val_accuracy: 0.9823\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 0.0673 - val_accuracy: 0.9831\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9797 - val_loss: 0.0674 - val_accuracy: 0.9823\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0584 - accuracy: 0.9808 - val_loss: 0.0667 - val_accuracy: 0.9800\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9808 - val_loss: 0.0653 - val_accuracy: 0.9808\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9802 - val_loss: 0.0683 - val_accuracy: 0.9792\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0576 - accuracy: 0.9810 - val_loss: 0.0654 - val_accuracy: 0.9815\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9808 - val_loss: 0.0665 - val_accuracy: 0.9823\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9805 - val_loss: 0.0712 - val_accuracy: 0.9823\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0617 - accuracy: 0.9784 - val_loss: 0.0662 - val_accuracy: 0.9823\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0604 - accuracy: 0.9805 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 0.0707 - val_accuracy: 0.9800\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0587 - accuracy: 0.9808 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9774 - val_loss: 0.0663 - val_accuracy: 0.9838\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.0700 - val_accuracy: 0.9815\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0584 - accuracy: 0.9808 - val_loss: 0.0658 - val_accuracy: 0.9823\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9810 - val_loss: 0.0668 - val_accuracy: 0.9800\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9808 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9802 - val_loss: 0.0684 - val_accuracy: 0.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.9805 - val_loss: 0.0648 - val_accuracy: 0.9815\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 0.9813 - val_loss: 0.0648 - val_accuracy: 0.9823\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9802 - val_loss: 0.0706 - val_accuracy: 0.9823\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0698 - val_accuracy: 0.9808\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9787 - val_loss: 0.0669 - val_accuracy: 0.9823\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.9810 - val_loss: 0.0654 - val_accuracy: 0.9815\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9808 - val_loss: 0.0660 - val_accuracy: 0.9808\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9820 - val_loss: 0.0645 - val_accuracy: 0.9823\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9815 - val_loss: 0.0686 - val_accuracy: 0.9815\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9802 - val_loss: 0.0649 - val_accuracy: 0.9823\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9828 - val_loss: 0.0643 - val_accuracy: 0.9815\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9815 - val_loss: 0.0641 - val_accuracy: 0.9823\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9813 - val_loss: 0.0677 - val_accuracy: 0.9800\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9823 - val_loss: 0.0658 - val_accuracy: 0.9800\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0644 - val_accuracy: 0.9815\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9826 - val_loss: 0.0640 - val_accuracy: 0.9838\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0530 - accuracy: 0.9828 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 0.0646 - val_accuracy: 0.9823\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 0.9815 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9828 - val_loss: 0.0655 - val_accuracy: 0.9815\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0536 - accuracy: 0.9826 - val_loss: 0.0662 - val_accuracy: 0.9800\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9818 - val_loss: 0.0655 - val_accuracy: 0.9800\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9818 - val_loss: 0.0643 - val_accuracy: 0.9846\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.9820 - val_loss: 0.0654 - val_accuracy: 0.9838\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9823 - val_loss: 0.0644 - val_accuracy: 0.9815\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9815 - val_loss: 0.0658 - val_accuracy: 0.9800\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9831 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0525 - accuracy: 0.9836 - val_loss: 0.0651 - val_accuracy: 0.9831\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 0.9831 - val_loss: 0.0637 - val_accuracy: 0.9823\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 0.0642 - val_accuracy: 0.9838\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9833 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0536 - accuracy: 0.9818 - val_loss: 0.0655 - val_accuracy: 0.9808\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0530 - accuracy: 0.9826 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9843 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.0642 - val_accuracy: 0.9831\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9826 - val_loss: 0.0671 - val_accuracy: 0.9838\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.9828 - val_loss: 0.0645 - val_accuracy: 0.9815\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9838 - val_loss: 0.0639 - val_accuracy: 0.9823\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9823 - val_loss: 0.0649 - val_accuracy: 0.9808\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9818 - val_loss: 0.0647 - val_accuracy: 0.9815\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9833 - val_loss: 0.0644 - val_accuracy: 0.9846\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.0636 - val_accuracy: 0.9838\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9828 - val_loss: 0.0638 - val_accuracy: 0.9831\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0521 - accuracy: 0.9823 - val_loss: 0.0636 - val_accuracy: 0.9838\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9836 - val_loss: 0.0674 - val_accuracy: 0.9831\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 0.9820 - val_loss: 0.0649 - val_accuracy: 0.9815\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9826 - val_loss: 0.0652 - val_accuracy: 0.9808\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.0638 - val_accuracy: 0.9831\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0518 - accuracy: 0.9823 - val_loss: 0.0643 - val_accuracy: 0.9815\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9831 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9836 - val_loss: 0.0642 - val_accuracy: 0.9823\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9831 - val_loss: 0.0641 - val_accuracy: 0.9846\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9836 - val_loss: 0.0646 - val_accuracy: 0.9846\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9823 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9841 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9838 - val_loss: 0.0653 - val_accuracy: 0.9815\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9838 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.0673 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0523 - accuracy: 0.9838 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9831 - val_loss: 0.0659 - val_accuracy: 0.9838\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9815 - val_loss: 0.0638 - val_accuracy: 0.9846\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9833 - val_loss: 0.0639 - val_accuracy: 0.9823\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0509 - accuracy: 0.9813 - val_loss: 0.0643 - val_accuracy: 0.9838\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.9823 - val_loss: 0.0633 - val_accuracy: 0.9854\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9843 - val_loss: 0.0642 - val_accuracy: 0.9815\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9833 - val_loss: 0.0632 - val_accuracy: 0.9846\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0497 - accuracy: 0.9856 - val_loss: 0.0634 - val_accuracy: 0.9831\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9831 - val_loss: 0.0639 - val_accuracy: 0.9831\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9836 - val_loss: 0.0633 - val_accuracy: 0.9831\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0492 - accuracy: 0.9836 - val_loss: 0.0640 - val_accuracy: 0.9846\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9831 - val_loss: 0.0634 - val_accuracy: 0.9838\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.0653 - val_accuracy: 0.9838\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 0.9826 - val_loss: 0.0652 - val_accuracy: 0.9831\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0521 - accuracy: 0.9823 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9826 - val_loss: 0.0644 - val_accuracy: 0.9831\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9831 - val_loss: 0.0708 - val_accuracy: 0.9800\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9838 - val_loss: 0.0643 - val_accuracy: 0.9831\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9826 - val_loss: 0.0641 - val_accuracy: 0.9854\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 0.9813 - val_loss: 0.0642 - val_accuracy: 0.9846\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0490 - accuracy: 0.9820 - val_loss: 0.0666 - val_accuracy: 0.9838\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 0.9831 - val_loss: 0.0631 - val_accuracy: 0.9838\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9826 - val_loss: 0.0667 - val_accuracy: 0.9815\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.9843 - val_loss: 0.0644 - val_accuracy: 0.9823\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9843 - val_loss: 0.0628 - val_accuracy: 0.9854\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.0685 - val_accuracy: 0.9838\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9841 - val_loss: 0.0670 - val_accuracy: 0.9815\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0480 - accuracy: 0.9851 - val_loss: 0.0637 - val_accuracy: 0.9831\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.9828 - val_loss: 0.0634 - val_accuracy: 0.9846\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9833 - val_loss: 0.0664 - val_accuracy: 0.9831\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9838 - val_loss: 0.0631 - val_accuracy: 0.9838\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 0.9841 - val_loss: 0.0648 - val_accuracy: 0.9823\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9815 - val_loss: 0.0659 - val_accuracy: 0.9815\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9826 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9828 - val_loss: 0.0697 - val_accuracy: 0.9808\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0722 - val_accuracy: 0.9800\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0494 - accuracy: 0.9831 - val_loss: 0.0646 - val_accuracy: 0.9846\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9838 - val_loss: 0.0637 - val_accuracy: 0.9846\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9831 - val_loss: 0.0631 - val_accuracy: 0.9854\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9843 - val_loss: 0.0628 - val_accuracy: 0.9838\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0464 - accuracy: 0.9851 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.9836 - val_loss: 0.0656 - val_accuracy: 0.9823\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.9828 - val_loss: 0.0632 - val_accuracy: 0.9846\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9828 - val_loss: 0.0651 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.keras\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer], verbose=0) #verbose=1 진행 화면 출력, verbose=0 진행 화면 출력 안함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9885\n",
      "Test accuracy: 0.9884615540504456\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
